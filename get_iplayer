#!/usr/bin/perl
#
# get_iplayer
#
# Lists and downloads BBC iplayer H.264 streams
#
# Author: Phil Lewis
# Email: iplayer (at sign) linuxcentre.net
# Web: http://linuxcentre.net
# License: GPLv3 (see LICENSE.txt)
# Date: June 30th 2008
#
my $version = 0.35;
#
# Supports: 
# * Downloading h.264/Mov/Quicktime streams from BBC iplayer site
# * Downloads *unencrypted* streams from BBC iplayer site
# * Re-jigs the mov file so that it can be streamed and start faster on some players
# * Resume downloads of partially downloaded files
# * Indexing of all available (i.e. listed) iplayer programs using new Atom feed
# * Caching of Index (default 4hrs)
# * Creation of a basic html index file
# * HTTP Proxy support (maybe broken for now on some proxies) (--proxy)
# * Regex search on programme name capability (makes it useful to run this from crontab)
# * Regex search on long programme description/episode capability (--long)
# * Regex search on channel and category (--channel, --category)
# * Save default options in ~/.iplayerrc (--save)
# * Tested on Linux (Fedora 6/7/8/9, Centos/RHEL 5, Ubuntu), and Windows (Activstate Perl)
#
# Requires:
# * perl 5.8
# * LWP (libwww-perl)
# * Perl XML::Twig
#
# Changes 0.35 - 20080630
# * Restructured downloading code
# * Now handles different versions better (i.e. Original, Signed) - tries to download next avaiable version type if one fails
# * Freevo FXD now has programmes sorted by channel
#
# Changes 0.34 - 20080629
# * Fixed duplicate listing bug when using --long
# * Added --quiet option
# * Added freevo symlink creation option --freevo
# * FXD file for freevo now uses standard Video plugin with a player wrapper script
# * Use logger function instead of print
#
# Changes 0.33 - 20080627
# * Now gets programme categories and channel from Atom feed
# * Only write the fxd and html files when writing cache
# * Channel and category search (regex) --channel --category
# * Fixed options saving/reading
# * Added download progress for Atom feed and movie header
#
# Changes 0.32 - 20080626
# * Now uses Atom feed rather than scraping website for programme info (much more reliable availability info)
# * Now requires Perl XML::Twig
# * Reduced cache expiry to 4 hrs (atom feed is now quicker to download and is more up to date)
# * version pid (pid) is no longer used for reference - only programme pid (urlpid) == $pid
# * Index now contains duration and start of availability date/time (Atom method only)
# * Allow fallback to scraping website for programme index
# * Removed deprecated XOR decoding
#
# Changes 0.31 - 20080624
# * Allow user to save options and specify default options in $HOME/.iplayerrc
# * Now uses long options also
#
# Changes 0.30 - 20080622
# * Generate FXD file for Freevo iplayer streaming plugin
# * Now can resume downloads and strema to STDOUT simultaneously (streaming always starts from beginning of programme)
# * Retry loop for getting index pages incase BBC site breaks like it just did
# 
# Changes 0.29 - 20080619:
# * Fixed streaming + file download mode
# * Added random numbers after some requests more like iphone
# * Added interrim checking of reported mp4 stream availability
# * Reduced index cacahe default to 12 hours
#
# Changes 0.28 - 20080619:
# * Update get_iplayer option (-u)
#
# Changes 0.27 - 20080619:
# * New option to allow streaming while downloading AND writing file to disk (allows you to watch while downloading while keeping a copy for later)
# * Sanity check to ensure we don't stream partially downloaded content
#
# Changes 0.26 - 20080618:
# * Fixed partial download resuming
# * Changed file suffix for partially downloaded files to <prog>.partial.mov to ease playback of partially downloaded movies 
#
# Changes 0.25 - 20080618:
# * Automatically re-arrange the atoms so that file can be streamed and start faster (used method from qt-faststart.c, v0.1 by Mike Melanson, melanson@pcisys.net)
# * Option -x allows STDOUT streaming during download (see usage)
# * All messages now go to STDERR
#
# Changes 0.24 - 20080618:
# * Now supports downloading of *unencrypted* streams from BBC Iplayer site - no more XOR nonsense
# * Downloads web bugs to whitelist our cookies on BBC servers
# * Downloads the file in blocks to avoid XOR
#
# Changes 0.23 - 20080617:
# * added binmode for decode sub also for windows to work properly
#
# Changes 0.22 - 20080616:
# * Detect null Content-Length header upon download - doesn't replace a partially d/loaded file if there is a failure
#
# Changes 0.21 - 20080616:
# * Added binmode for Windows file writing - fixes bug where windows would not decode properly
#
# Changes 0.20 - 20080615:
# * Remaining time reported in hrs/mins/secs
# * Added sort-into-subdirectories option to create subdirectories for programmes
#
# Changes 0.19 - 20080614:
# * Fixed PID searching
# * Now supports restarting downloads of incomplete files
# * Fixed swap detection bug where moov atom started on even byte offset
# * Remaining time added
# * Fixed kbps (not misreported kBps anymore)
#
# Changes 0.18 - 20080613:
# * Auto detection of byte-swapping
# * Auto detection of XOR sequence
#
# Changes 0.17 - 20080612:
# * Non-UK error detection
# * Not available error detection
#
# Changes 0.16 - 20080612:
# * Fixed HTML output links 
# * Sanitized data structures
# * Now parses days/hours left
# * Fixed XOR % completion calculation
# * Reads IPLAYER_OUTDIR environment
#
# Changes 0.15 - 20080611:
# * Fixed duplicate programme removal
# * Now supports different programme type downloads (i.e. Original, Signed, etc)
# * Changes hashes to give more meaningful names
# * Test option (-t) will now determine the programme type
# * Increased speed of XOR decoder by reading in 1024 blocks at a time
#
# Changes 0.14 - 20080611:
# * Not using curl anymore - seems to fail through some firewalls mysteriously and will not work on Ubuntu
# * Using LWP for all http requests
# * Fixed indexing sort bug
#

use strict;
use URI;
use IO::Socket;
use Getopt::Long;
use Fcntl;
use File::stat;
use File::Copy;
use IO::Seekable;
use LWP::UserAgent;
#use LWP::Debug qw(+);
use HTTP::Cookies;
use HTTP::Headers;
use XML::Twig;
my $DEBUG = 0;
$|=1;
my %opt = ();

# Print to STDERR
sub logger(@) {
	print STDERR @_[0] if ! $opt{quiet};
}

sub usage {
	logger <<EOF;
Usage:
  List Programmes:       get_iplayer [-v] [-l] [-e <secs>] [-p <proxy_url>]
  Download files:        get_iplayer [<options>] <regex|index|pid|url> [<regex|index|pid|url>]
  Stream Downloads:      get_iplayer [-v] [-x] [-n] [-p <proxy_url>] <regex|index|pid|url> | mplayer -cache 2048 -
  Update get_iplayer:    get_iplayer -u

Options:
 -l, --long           Show/search long programme descriptions
     --channel        Only search in specified channel(s) - use a sub-string or regex
     --category       Only search in specified category - use a sub-string or regex
 -f, --flush          Flush cache
 -o, --output <dir>   Download output directory
 -s, --subdir         Downloaded files into Programme name subdirectory
 -p, --proxy <url>    Web proxy URL spec
 -x, --stdout         Additionally stream to STDOUT (so you can pipe output to a player)
 -n, --nowrite        No writing of file to disk (use with -x to prevent a copy being stored on disk)
 -e, --expiry <secs>  Cache expiry in seconds (default 12hrs)
 -t, --test           Test only - no download (will show programme type)
 -w, --whitespace     Keep whitespace (and escape chars) in filenames
     --freevo <file>  Create symlink to <file> once we have the header of the download
 -v, --verbose        Verbose
 -u, --update         Update get_iplayer if a newer one exists
 -h, --help           Help
 -q, --quiet          No output
     --scrape         Use old web page scraping method to get programme index
     --save           Save these options as default in .iplayerrc
EOF
	exit 1;
}

# Get cmdline params
my $save;
my $optfile = "$ENV{HOME}/.iplayerrc";

# Parse options if we're not saving options
read_options_file() if ! grep /\-\-save/, @ARGV;

# cmdline opts take precedence
GetOptions(
        "help|h"                        => \$opt{help},
        "long|l"                        => \$opt{long},
        "verbose|v"                     => \$opt{verbose},
        "flush|f"                       => \$opt{flush},
        "output|o=s"                    => \$opt{output},
        "proxy|p=s"                     => \$opt{proxy},
        "stdout|stream|x"               => \$opt{stdout},
        "subdirs|subdir|s"              => \$opt{subdir},
        "no-write|nowrite|n"            => \$opt{nowrite},
        "expiry|e=n"                    => \$opt{expiry},
        "test|t"                        => \$opt{test},
        "whitespace|ws|w"               => \$opt{whitespace},
        "update|u"                      => \$opt{update},
        "debug"                         => \$opt{debug},
        "scrape"			=> \$opt{scrape},
	"save"				=> \$save,
	"channel=s"			=> \$opt{channel},
	"category=s"			=> \$opt{category},
	"q|quiet"			=> \$opt{quiet},
	"freevo=s"			=> \$opt{freevo},
) || die usage();
usage() if $opt{help};
my $DEBUG = $opt{debug};

# Save opts if specified
save_options_file() if $save;

# Options
my $download_dir 	= $opt{output} || "$ENV{IPLAYER_OUTDIR}" || '.'; # Where downloads will be written
my $webfile 		= $download_dir.'/iplayer.html'; # Where html file is generated
my $fxdfile 		= $download_dir.'/folder.fxd'; # Where freevo will scan for FXD file
my $get_iplayer_stream	= 'get_iplayer_freevo_wrapper';	# Location of wrapper script for streaming with mplayer/xine on freevo
my $cookiejar		= "/tmp/iplayer_cookie_$$";
my $cachefile		= '/tmp/iplayer.cache';
my $cache_secs 		= $opt{expiry} || 14400;

# URLs
my $search_page_prefix	= 'http://www.bbc.co.uk/iplayer/atoz/?filter=azgroup%3A*&start=';
my $atom_feed_url	= 'http://www.bbc.co.uk/iplayer/atom/available.xml';
my $pid_page_url_prefix	= 'http://www.bbc.co.uk/iplayer/page/item/';
my $web_bug_2_url	= 'http://www.bbc.co.uk/iplayer/framework/img/o.gif?';
my $url_download_prefix	= 'http://www.bbc.co.uk/mediaselector/3/auth/iplayer_streaming_http_mp4';
my $prog_page_prefix	= 'http://www.bbc.co.uk/programmes';
my $version_url		= 'http://linuxcentre.net/get_iplayer/VERSION-get_iplayer';
my $update_url		= 'http://linuxcentre.net/get_iplayer/get_iplayer';

# User Agents
my %user_agent = (
  	coremedia	=> "Apple iPhone v1.1.1 CoreMedia v1.0.0.3A110a",
  	safari		=> "Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A110a Safari/419.3",
  	update		=> "get_iplayer updater (v${version} - $^O)",
);

# Sanity check some conflicting options
if ($opt{nowrite} && (!$opt{stdout})) {
  logger "ERROR: Cannot download to nowhere\n";
  exit 1;
}

# Programme data structure
# $prog{$urlpid} = {
#	'index'		=> <index number>,
#	'name'		=> <programme short name>,
#	'episode'	=> <Episode info>,
#	'desc'		=> <Long Description>,
#	'available'	=> <Date/Time made available>,
#	'longname'	=> <Long name (only parsed in stage 1)>,
#	'duration'	=> <duration in HH:MM:SS>
#	'versions'	=> <list of non-'Original' versions, e.g Signed>
#	'thumbnail'	=> <programme thumbnail url>
#	'channel	=> <TV channel>
#	'categories'	=> <Categories>
#};
my %prog;
# Hash to obtain pid given an index
my %index_pid;
my $now;
my $childpid;

# Web proxy
my $proxy_url = $opt{proxy} || $ENV{HTTP_PROXY} || $ENV{http_proxy} || '';
logger "INFO: Using Proxy $proxy_url\n" if $proxy_url;

# Update this script if required
if ($opt{update}) {
  update_script();
}

# Check for valid dload dir
die ("ERROR: $download_dir does not exist\n") if ! -d $download_dir;

# Flush cache if required
unlink ($cachefile) if $opt{flush};

# Get stream links from BBC iplayer site or from cache (also populates all hashes)
get_links();

# Print list of programmes if we're not downloading anything
if ( ! $ARGV[0] ) {
  list_progs( get_regex_matches( '.*' ) );
  #list_progs( sort { $a <=> $b } keys %index_pid );
  exit 0;
}

# Parse remaining args
my @download_list;
for ( @ARGV ) {
  chomp();

  # If Numerical value
  if ( /^[\d]+$/ ) {
    push @download_list, $_;

  # If PID then find matching programmes with this PID
  } elsif ( /^.*b0[a-z,0-9]{6}.*$/ ) {
    s/^.*(b0[a-z,0-9]{6}).*$/$1/g;
    push @download_list, get_regex_matches( $1 );

  # Else assume this is a programme name regex
  } else {
    push @download_list, get_regex_matches( $_ );
  }
}

# Display list for download
logger "Download List:\n" if @download_list;
list_progs( @download_list );

# Do the downloads based on list of index numbers
download_programme( $index_pid{$_} ) for (@download_list);

exit 0;





# Lists progs given an array of index numbers
sub list_progs {
	for (@_) {
		my $pid = $index_pid{$_};
		my $desc = "- $prog{$pid}{desc}" if $opt{long};
		logger "$_: $prog{$pid}{name} - $prog{$pid}{episode}, $prog{$pid}{available}, $prog{$pid}{duration}, '$prog{$pid}{channel}', $prog{$pid}{categories} $prog{$pid}{versions} $desc\n";
	}
	logger "\n";
	return 0;
}


# Get matching programme index numbers using supplied regex
sub get_regex_matches {
	my $download_regex = shift;
	my %download_hash;
	my $channel_regex = $opt{channel} || '.*';
	my $category_regex = $opt{category} || '.*';

	for (keys %index_pid) {
		my $pid = $index_pid{$_};

		# Only include programmes matching channels and catoegory regexes
		if ( $prog{$pid}{channel} =~ /$channel_regex/i && $prog{$pid}{categories} =~ /$category_regex/i ) {

			# Search prognames/pids while excluding channel_regex and category_regex
			$download_hash{$_} = 1 if (
				$prog{$pid}{name} =~ /$download_regex/i
				|| ( $pid =~ /$download_regex/i && $download_regex =~ /b00/ )
				|| ( $pid =~ /$download_regex/i && $download_regex =~ /b00/ )
			);
			# Also search long descriptions and episode data if -l is specified
			$download_hash{$_} = 1 if (
				$opt{long} 
				&& 
				( $prog{$pid}{desc} =~ /$download_regex/i 
				  || $prog{$pid}{episode} =~ /$download_regex/i
				)
			);
		}
	}
	return sort {$a <=> $b} keys %download_hash;
}


sub get_links_all {
	my $pageno = 1;
	my $pid;
	my @html;
	my $valid = 1;
	logger "INFO: Getting Index page\n";
	# Setup User agent
	my $ua = LWP::UserAgent->new;
	$ua->timeout([10]);
	$ua->proxy( ['http'] => $proxy_url );
	$ua->agent( $user_agent{safari} );

	# Loop while we still get stream links
	do {
		my $retries = 3;
		logger "DEBUG: Getting ${search_page_prefix}${pageno}\n" if $DEBUG;
		do {
			# Get index pages
			my $res = $ua->request( HTTP::Request->new( GET => "${search_page_prefix}${pageno}" ) );
			@html = split /\n/, $res->content;
			# Set valid flag if we have progdata in this page
			$valid = 0 if ! grep(/version_pid=\w+">[^<]+<\/a>/, @html);
			if ( ! $res->is_success ) {
				logger "WARNING: Failed to get programme index page ${pageno} from iplayer site\n";
				$retries--;
				sleep 1;
				logger "X";
				exit 2 if $retries == 0;
			} else {
				$retries = 0;
				logger ".";
			} 
		} until ($retries == 0);

		# Get the complete URL
		# e.g.:  <a class="resultlink" href="/iplayer/page/item/b00bwky1.shtml?filter=azgroup%3A%2A&amp;start=33&amp;scope=iplayeratoz&amp;version_pid=b00c3kcd">Wild China</a>
		# Get Episode info: Info follows for upto 40 lines after matched line, e.g.
		# ........
		#		<span class="available">
		#		
		#			16 hours left
		#		
		#		</span>
		#	</p>
		# .........
		#                                       <div class="resultSynopsis">
		#                <p class="title">
		#                        <strong>Stake Out</strong>
		#                        <span class="divider">|</span>
		#                        Episode 7
		#                </p>
		#                <p class="description">Children's hidden camera game show that finds out how well kids know their mates. The Prince of Askabar visits Edinb
		#        </div>

		# parse @html array for episode info
		while (@html) {
			my $progdata;
			chomp( my $line = shift @html );
			# If this line has a version_pid line (i.e. one with anchor text in it)
			if ( $line =~ /version_pid=\w+">[^<]+<\/a>/ ) {
				# Need version_pid & url (note these are different pids)
				# Get url and extract the URLPID and PID
				$line =~ s/^.*href=\".*(b0.{6})\.shtml\?.*version_pid=(\w+)">[^<]+<\/a>/$1 $2/;
				my $pid = $1;
				# get next 40 lines into a single string
				$progdata = '';
				$progdata .= shift @html for (1..40);
				# Extract prog data
				$progdata =~ s!^.*<span class=\"available\">\s*([\w ]+)\s*</span>.*<p class=\"title\">\s*<strong>(.+)</strong>.+</span>\s*([^<]+)</p>.*<p class=\"description\">([^<]+).*$!$1|$2|$3|$4!g;
				# Remove unwanted whitespace
				$progdata =~ s/\s*$//g;
				$progdata =~ s/\s*\|/|/g;
				$progdata =~ s/\|\s*/|/g;
				logger "pid|available|prog|episode|desc = '$pid|$progdata'\n" if $opt{verbose};
				my ($pid, $available, $name, $episode, $desc) = split /\|/, "$pid|$progdata";;
				# Create data structure with prog data
				$prog{$pid} = {
					'name'		=> $name,
					'versions'	=> '<versions>',
					'episode'	=> $episode,
					'desc'		=> $desc,
					'available'	=> $available,
					'duration'	=> '<duration>',
					'thumbnail'	=> '<thumbnail>',
					'channel'	=> '<channel>',
					'categories'	=> '<categories>',
				};
				logger "  $name ($pid) - $episode - $desc\n" if $opt{verbose};
			}
		}
		# Next page
		$pageno++;
	} while ($valid);

	# Add index field based on alphabetical sorting by prog name
	my $index = 1;
	my @prog_pid;
	# Create unique array of '<progname|pid>'
	push @prog_pid, "$prog{$_}{name}|$_" for (keys %prog);
	# Sort by progname and index 
	for (sort @prog_pid) {
		# Extract pid
		my $pid = (split /\|/)[1];
		$index_pid{$index} = $pid;
		$prog{$pid}{index} = $index;
		$index++;
	}
	logger "\n";
	return 0;
}


sub get_links_atom {

	my $xml;
	my $valid = 1;
	logger "INFO: Getting Index Feed\n";
	# Setup User agent
	my $ua = LWP::UserAgent->new;
	$ua->timeout([10]);
	$ua->proxy( ['http'] => $proxy_url );

	# Download index feed
	my $retries = 3;
	do {
		# Get index pages
		$now = time();
		$xml = download_block(undef, $atom_feed_url, $ua);
		if (! $xml) {
			logger "WARNING: Failed to get programme index feed from iplayer site\n";
			$retries--;
			exit 2 if $retries == 0;
			sleep 1;
		} else {
			$retries = 0;
		} 
	} until ($retries == 0);

	# Parse XML
	logger "\nINFO: Parsing Programme Data\n";
	my $twig = XML::Twig->new();
	$twig->parse($xml);
	my $root = $twig->root;

#  <entry>
#    <title type="text">Take a Bow: Street Feet on the Farm</title>
#    <id>tag:bbc.co.uk,2008:PIPS:b008pj3w</id>
#    <updated>2008-06-17T16:04:51Z</updated>
#    <content type="xhtml" xmlns:xhtml="http://www.w3.org/1999/xhtml">
#      <xhtml:div>
#        <xhtml:a href="http://www.bbc.co.uk/iplayer/page/item/b008pj3w.shtml">
#          <xhtml:img src="http://www.bbc.co.uk/iplayer/images/episode/b008pj3w_150_84.jpg" alt="Take a Bow: Street Feet on the Farm" />
#        </xhtml:a>
#        <xhtml:br/>A celebration of young performing talents across the UK. Children from Cardiff become streetwise on the farm.
#      </xhtml:div>
#    </content>
#    <link rel="enclosure" href="http://www.bbc.co.uk/iplayer/images/episode/b008pj3w_150_84.jpg" type="image/jpeg" title="Take a Bow: Street Feet on the Farm" length="0" />
#    <link rel="enclosure" href="http://www.bbc.co.uk/iplayer/images/episode/b008pj3w_303_170.jpg" type="image/jpeg" title="Take a Bow: Street Feet on the Farm" length="0" />
#    <link rel="enclosure" href="http://www.bbc.co.uk/iplayer/images/episode/b008pj3w_512_288.jpg" type="image/jpeg" title="Take a Bow: Street Feet on the Farm" length="0" />
#    <media:thumbnail url="http://www.bbc.co.uk/iplayer/images/episode/b008pj3w_150_84.jpg" width="150" height="84" time="" />
#        <!-- Related site - not implemented yet -->
#        <!--link href="http://www.bbc.co.uk/some_brand_site" rel="related" type="text/html" /-->
#    <category term="100001" scheme="http://www.bbc.co.uk/pips#category" label="Children's"/>
#    <category term="200001" scheme="http://www.bbc.co.uk/pips#category" label="Activities"/>
#    <category term="100001PT011" scheme="http://www.bbc.co.uk/cpe#category" label="Children's, Performances &amp; Events"/>
#    <category term="letterT" scheme="http://www.bbc.co.uk/cpe#category" label="T"/>
#    <author>
#      <name>CBeebies</name>
#      <uri>http://www.bbc.co.uk/iplayer/framework/img/ch/cbeebies.gif</uri>
#    </author>
#    <duration>00:05:00</duration>
#  </entry>


	my ( $name, $episode, $desc, $pid, $available );
	foreach my $entry ($root->children('entry')) {

		# parse name: episode, e.g. Take a Bow: Street Feet on the Farm
		$name = $entry->first_child_text('title');
		$episode = $name;
		$name =~ s/^(.*): .*$/$1/g;
		$episode =~ s/^.*: (.*)$/$1/g;

		# tag:bbc.co.uk,2008:PIPS:b008pj3w
		$pid = $entry->first_child_text('id');
		$pid =~ s/^.*PIPS:(.*)$/$1/g;

		# 2008-06-22T05:01:49Z
		$available = $entry->first_child_text('updated');
		$available =~ s/^(\d{4}\-\d\d\-\d\d).*(\d\d:\d\d:\d\d).*$/$1 $2/g;

		# parse embedded xhtml
		$desc = $entry->first_child('content')->child_text(0,'xhtml:div');
		$desc =~ s|^.*<xhtml:br/>(.*)$|$1|g;
		$desc =~ s/\n/ /g;
		$desc =~ s/^\s*(.*)\s*$/$1/g;
		
		# Parse the categories into hash
		my %category;
		for (my $i = 0; $i < $entry->children_count('category'); $i++) {
			$category{ $entry->child($i, 'category')->att('term') } = $entry->child($i,'category')->att('label');
		}		
		# We only so far have h264 versions for Signed and Original AFAIK, add entry for both of these (instead of version pid)
		my @versions;
		my @categories;
		# If another version type does not occur in the categories then this is Original only
		#push @versions, 'Original'; # if ( ! grep /^version.*/, keys %category );
		# Add an entry for signed version if it exists
		for (keys %category) {
			push @versions, $category{$_} if $_ =~ /^version/ && $category{$_} eq 'Signed';
			push @categories, $category{$_} if /^\d{6}$/;
		}

                my $elt;
		my $channel;
		$channel  = $elt->next_elt()->first_child_text() if ( $elt = $entry->first_child('author') );

		# build data structure
		$prog{$pid} = {
			'name'		=> $name,
			'versions'	=> (join ',', @versions),
			'episode'	=> $episode,
			'desc'		=> $desc,
			'available'	=> $available,
			'duration'	=> $entry->first_child_text('duration'),
			'thumbnail'	=> $entry->first_child('media:thumbnail')->att('url'),
			'channel'	=> $channel,
			'categories'	=> (join ',', @categories),
		};
	}
	# Add index field based on alphabetical sorting by prog name
	my $index = 1;
	my @prog_pid;
	# Create unique array of '<progname|pid>'
	push @prog_pid, "$prog{$_}{name}|$_" for (keys %prog);
	# Sort by progname and index 
	for (sort @prog_pid) {
		# Extract pid
		my $pid = (split /\|/)[1];
		$index_pid{$index} = $pid;
		$prog{$pid}{index} = $index;
		$index++;
	}

	return 0;
}



sub get_links {
	my @cache;
	my $now = time();

	# Open cache file (need to verify we can even read this)
	if ( open(CACHE, "< $cachefile") ) {
		@cache = <CACHE>;
		close (CACHE);
	}

	# if a cache file doesn't exist/corrupted or original file is older than 2 mins then download new data
	if ( ($cache[0] =~ /^0$/) || (! -f $cachefile) || ($now >= ( stat($cachefile)->mtime + $cache_secs )) ) {

		if ($opt{scrape}) {
			# Use A-Z,0-9 searchine '*' instead - less pages
			get_links_all();
		} else {
			# Get index from Atom feed
			get_links_atom();
		}
		
		# Open cache file for writing
		if ( open(CACHE, "> $cachefile") ) {
			for (sort {$a <=> $b} keys %index_pid) {
				my $pid = $index_pid{$_};
				print CACHE "$_|$prog{$pid}{name}|$pid|$prog{$pid}{available}|$prog{$pid}{episode}|$prog{$pid}{versions}|$prog{$pid}{duration}|$prog{$pid}{desc}|$prog{$pid}{channel}|$prog{$pid}{categories}\n";
			}
			close (CACHE);
			# Make sure anyone can read/write file (security risk here!!)
			chmod 0777, $cachefile;

			# Also write HTML and FXD files
			create_html( sort {$a <=> $b} keys %index_pid );
			create_fxd ( sort {$a <=> $b} keys %index_pid );

		} else {
			logger "WARNING: Couldn't open cache file for writing\n";
		}


	# Else read from cache
	} else {
		for (@cache) {
			# Populate %prog from cache
			chomp();
			my ($index, $name, $pid, $available, $episode, $versions, $duration, $desc, $channel, $categories) = split /\|/;
			# Create data structure with prog data
			$prog{$pid} = {
				'index'		=> $index,
				'name'		=> $name,
				'episode'	=> $episode,
				'desc'		=> $desc,
				'available'	=> $available,
				'duration'	=> $duration,
				'versions'	=> $versions,
				'channel'	=> $channel,
				'categories'	=> $categories,
			};
			$index_pid{$index}	= $pid;
		}
	}
	return 0;
}


# Usage: download_programme (<pid>)
sub download_programme {
	my $pid = shift;
	
	# Create a full URL from the PID specified
	my $page = $pid_page_url_prefix.$pid.'.shtml';
	logger "INFO: Attempting to Download: $prog{$pid}{name} - $prog{$pid}{episode}\n";

	logger "INFO: Stage 1 URL = $page\n" if $opt{verbose};
	logger "\rGetting iplayer programme page        " if ! $opt{verbose};

	# Switch off automatic redirects
	my $ua = LWP::UserAgent->new( requests_redirectable => [] );
	# Setup user agent
	# 30 second request timeout
	$ua->timeout([30]);
	$ua->proxy( ['http'] => $proxy_url );
	$ua->cookie_jar( HTTP::Cookies->new( file => $cookiejar, autosave => 1, ignore_discard => 0 ) );

	# Stage 1: get PID and set cookie
	$ua->agent( $user_agent{safari} );
	# send request
	my $res = $ua->request( HTTP::Request->new( GET => $page ) );
	if ( ! $res->is_success ) {
		logger "\rERROR: Failed to get programme ID from iplayer site\n\n";
		return 7;
	}
	my @content = split /\n/, $res->content;
	# Non-UK detection
	if ( grep /only available to play in the UK/i, @content ) {
		logger "\nERROR: This service will only work from the UK or via a UK based web proxy.\n";
		exit 3;
	}

	# Parse if programme available
	#    iplayer_streaming_http_mp4 : [
	#    ],  ### This part is empty == no mov version yet
	if ( grep /iplayer_streaming_http_mp4 : \[\s+\],/i, $res->content ) {
		logger "\rWARNING: Programme is reported as not yet ready for download\n\n";
		# Will return from here once satisfied that this test is reliable
		#return 11;
	} else {
	        logger "\rINFO: Programme is reported as ready for download\n";
	}

	# Extract Long Name, e.g.: iplayer.prog = " The Really Wild Show: Series 21"; 
	chomp( $prog{$pid}{longname} = (grep /^\s*iplayer.prog = \".+"/, @content)[0] );
	$prog{$pid}{longname} =~ s/^\s*iplayer.prog = "\s*(.+)\s*".*$/$1/g;

	# Get type => verpid
	my %verpid_types = get_ver_pid_types( @content );
	my $url_2;

	# Do this for each version tried in this order (if they appaered in the content)
	for my $type ( qw/ Original Signed AudioDescribed OpenSubtitled Shortened Lengthened Other / ) {

		# Change $verpid to 'Original' type if it exists, then Used 'Signed' otherwise
		if ( grep /^$type$/, keys %verpid_types ) {
			logger "INFO: Checking existence of type $type\n";
			$prog{$pid}{type} = $type;
			# Create url with appended 6 digit random number
			my $url_1 = ${url_download_prefix}.'/'.$verpid_types{$type}.'?'.(sprintf "%06.0f", 1000000*rand(0)).'%20';

			logger "INFO: Stage 2 Type = $prog{$pid}{type}\n" if $opt{verbose};
			logger "INFO: Stage 2 URL = $url_1\n" if $opt{verbose};

			# Get these web bugs to whitelist our cookie
			get_web_bugs($ua, @content);

			$url_2 = get_stream_download_url( $ua, $url_1 );
		}
		# Break out of loop if we have an actual URL
		last if $url_2;
	}

	# Report error if no versions are available
	if ( ! $url_2 ) {
		logger "ERROR: No versions exist for download";
		return 14;
	}	


	# Determine the correct filenames for this download
	my $file_prefix = generate_download_filename_prefix( $pid );
	logger "\rINFO: File name prefix = $file_prefix                 \n";
	my $file_done = "${download_dir}/${file_prefix}.mov";
	my $file = "${download_dir}/${file_prefix}.partial.mov";
	if ( -f $file_done ) {
		logger "ERROR: File already exists\n\n";
		return 1;
	}	

	# Skip from here if we are only testing downloads
	return 0 if $opt{test};

	# Do the h.264 download
	download_h264_stream( $ua, $url_2, $file, $file_done );

	return 0;
}


# Actually do the h.264 downloading
sub download_h264_stream {
		my ( $ua, $url_2, $file, $file_done ) = @_;

		# Stage 3a: Download 1st byte to get exact file length
		logger "INFO: Stage 3 URL = $url_2\n" if $opt{verbose};

                # Determine offset for continuation dowload
		my $h = new HTTP::Headers(
			'User-Agent'	=> $user_agent{coremedia},
			'Accept'	=> '*/*',
			'Range'		=> 'bytes=0-1',
		);
		my $req = HTTP::Request->new ('GET', $url_2, $h);
		my $res = $ua->request($req);
		# e.g. Content-Range: bytes 0-1/181338136
		my $file_len = $res->header("Content-Range");
                $file_len =~ s|^bytes 0-1/(\d+).*$|$1|;
		logger "INFO: Download File Length $file_len\n" if $opt{verbose};

		# Get ftyp+wide header etc
		my $mdat_start = 0x1c;
		$now = time();
		my $buffer = download_block(undef, $url_2, $ua, 0, $mdat_start + 4);
		# Get bytes upto (but not including) mdat atom start -> $header
		$now = time();
		my $header = download_block(undef, $url_2, $ua, 0, $mdat_start - 1, $file_len);

		# Detemine moov start
		# Get mdat_end_offset_chars from downloaded block
		my $mdat_end_offset_chars = substr($buffer, $mdat_start, 4);
		my $mdat_end_offset = bytestring_to_int($mdat_end_offset_chars);
		logger "mdat_end_offset = ".get_hex($mdat_end_offset_chars)." = $mdat_end_offset\n" if $opt{verbose};
		logger "mdat_end_offset (decimal) = $mdat_end_offset\n" if $opt{verbose};
		# The MOOV box starts one byte after MDAT box ends
		my $moov_start = $mdat_start + $mdat_end_offset;


		## scan 2nd level atoms in moov atom until we get stco atom(s)
		# We can skip first 8 bytes (moov atom header)
		#my $i = 8;
		#while( $i < $moov_length - 4 ) {
		#  my $atom_len = bytestring_to_int( substr($moovdata, $i, 4) );
		#  my $atom_name = substr($moovdata, $i+4, 4);
		#  logger "Parsing atom: $atom_name, length: $atom_len\n";
		#  # Increment $i by atom_len to get next atom
		#  $i += $atom_len;
		#}

		# If we have partial content and wish to stream, resume the download & spawn off STDOUT from existing file start 
		# Sanity check - we cannot support downloading of partial content if we're srtreaming also. 
                if ( $opt{stdout} && (! $opt{nowrite}) && -f $file ) {
                  logger "WARNING: Partially downloaded file exists, streaming will start from the beginning of the programme\n";
                  # Don't do usual streaming code
                  $opt{stdout} = 0;
                  $childpid = fork();
                  if (! $childpid) {
			# Child starts here
                	logger "INFO: Streaming directly for partially downloaded file $file\n";
			if ( ! open( STREAMIN, "< $file" ) ) {
				logger "INFO: Cannot Read partially downloaded file to stream\n";
				exit 4;
			}
			my $outbuf;
			# Write out until we run out of bytes
			my $bytes_read = 65536;
			while ( $bytes_read == 65536 ) { 
				$bytes_read = read(STREAMIN, $outbuf, 65536 );				
				#logger "INFO: Read $bytes_read bytes\n";
				print STDOUT $outbuf;
			}
			close STREAMIN;
			logger "INFO: Stream thread has completed\n";
                	exit 0;
		  }
                }

                # Open file if required
                my $fh = open_file_append($file);

                # If the partial file already exists, then redownload from the correct mdat/download offset
                my $restart_offset = $mdat_start;
		my $moovdata;
                my $moov_length;
                # If we have a too-small-sized file and not stdout and not no-write then this is a partial download
                if (-f $file && (! $opt{stdout}) && (! $opt{nowrite}) && stat($file)->size > ($moov_length+$mdat_start) ) {
		  # Get moovlength from server
		  $now = time();
		  $moov_length = bytestring_to_int( download_block( undef, $url_2, $ua, $moov_start, $moov_start+3 ) );
		  # Calculate new start offset (considering that we've put moov first in file)
                  $restart_offset = stat($file)->size - $moov_length;
                  logger "INFO: Restarting download from $restart_offset\n";
                }


                # If we have no existing file, a file which doesn't yet even have the moov atom, or using stdout (or no-write option)
                if ( $opt{stdout} || $opt{nowrite} || stat($file)->size < ($moov_length+$mdat_start) ) {
		  # get moov chunk into memory
		  $now = time();
		  $moovdata = download_block( undef, $url_2, $ua, $moov_start, (${file_len}-1) );
		  # Process the moov data so that we can relocate it (change the chunk offsets that are absolute)
                  $moov_length = relocate_moov_chunk_offsets( $moovdata );
  		  # write moov atom to file next (yes - were rearranging the file - moov+header+mdat - not header+mdat+moov)
		  logger "INFO: Appending moov+ftype+wide atoms to $file\n" if $opt{verbose};
		  # Write moov atom
                  print $fh $moovdata if ! $opt{nowrite};
		  print STDOUT $moovdata if $opt{stdout};
                  # Write header atoms (ftyp, wide)
                  print $fh $header if ! $opt{nowrite};
		  print STDOUT $header if $opt{stdout};
                }

                # Create symlink for freevo if required
                if ( $opt{freevo} ) {
                	# remove old symlink
                	unlink $opt{freevo} if -l $opt{freevo};
                	symlink $file, $opt{freevo};
                }
                
		# Download mdat in 32MB blocks
		$now = time();
		my $chunk_size = 0x2000000;
		for ( my $s = $restart_offset; $s < ${moov_start}-1; $s+= $chunk_size ) {
		  # get mdat chunk into file
		  my $retcode;
		  my $e;
		  # Get block end offset
		  if ( ($s + $chunk_size - 1) > (${moov_start}-1) ) {
		    $e = $moov_start - 1;
                  } else {
                    $e = $s + $chunk_size - 1;
                  }
                  # Get block from URL and append to $file
                  if ( download_block($file, $url_2, $ua, $s, $e, $file_len, $fh ) ) {
                    logger "ERROR: Could not download block $s - $e from $file\n\n";
                    return 9;
                  }
		}

		# Should now be able to concatenate header.block + mdat.block + moov.block to get movie!
		logger "INFO: Downloaded $file_done\n";
		unlink $cookiejar;
		# Moving file into place as complete (if not stdout)
                move($file, $file_done) if ! $opt{stdout};

                return 0;
}



# Get streaming mp4 URL
sub get_stream_download_url {
		my $ua = shift;
		my $url_1 = shift;
		
		# Stage 2: e.g. "Location: http://download.iplayer.bbc.co.uk/iplayer_streaming_http_mp4/121285241910131406.mp4?token=iVXexp1yQt4jalB2Hkl%2BMqI25nz2WKiSsqD7LzRmowrwXGe%2Bq94k8KPsm7pI8kDkLslodvHySUyU%0ApM76%2BxEGtoQTF20ZdFjuqo1%2B3b7Qmb2StOGniozptrHEVQl%2FYebFKVNINg%3D%3D%0A"
		logger "\rGetting iplayer download URL         " if ! $opt{verbose};
		my $h = new HTTP::Headers(
			'User-Agent'	=> $user_agent{coremedia},
			'Accept'	=> '*/*',
			'Range'		=> 'bytes=0-1',
		);
		my $req = HTTP::Request->new ('GET', $url_1, $h);
		# send request
		my $res = $ua->request($req);
		# Get resulting Location header (i.e. redirect URL)
		my $url_2 = $res->header("location");
		if ( ! $res->is_redirect ) {
			logger "ERROR: Failed to get redirect from iplayer site\n\n";
			return '';
		}
		# Extract redirection Location URL
		$url_2 =~ s/^Location: (.*)$/$1/g;
		# If we get a Redirection containing statuscode=404 then this prog is not yet ready
		if ( $url_2 =~ /statuscode=404/ ) {
			logger "\rERROR: Programme is not yet ready for download\n";
			return '';
		}

		return $url_2;
}



# Given page content, extract the Versions and Pids and return in a hash: Versions => Pid
sub get_ver_pid_types {
	my @content = @_;
	# Extract version pid and type (i.e. Signed, Original, etc)
	chomp( my @type_pid = grep /(type|pid )      :/, @content );
	# Remove tags
	s/^.*'(.+)'.*$/$1/g for @type_pid;
	# Get hash of pid => type
	my %types = @type_pid;
	logger "INFO: Types available: ", join(', ', %types), "\n" if $opt{verbose};
	return %types;
}


# Generate the download filename prefix given a pid
sub generate_download_filename_prefix {
	my $pid = shift;
	# Create a filename
	my $file = "$prog{$pid}{longname} - $prog{$pid}{episode} ${pid} $prog{$pid}{type}";
	# Replace slashes with _ regardless
	$file =~ s/[\\\/]/_/g;
	# Sanitize by default
	$file =~ s/\s/_/g if ! $opt{whitespace};
	$file =~ s/[^\w_-]//gi if ! $opt{whitespace};

	# Don't create subdir if we are only testing downloads
	# Create a subdir for programme sorting option
	if ( $opt{subdir} && ! $opt{test} ) {
	        my $subdir = "$prog{$pid}{longname}";
                $subdir =~ s/[\\\/]/_/g;
                $subdir =~ s/\s/_/g if ! $opt{whitespace};
                $subdir =~ s/[^\w_-]//gi if ! $opt{whitespace};
                $file = "${subdir}/${file}";
                # Create dir if it does not exist
                mkdir("${download_dir}/${subdir}") if ! -d "${download_dir}/${subdir}";
        }

        return $file;
}



sub get_web_bugs {
	my $ua = shift;
	my @content = @_;

	# Parse this to get o.gif for stats web bug
	#<script type="text/javascript">
	#var i = new Image(1,1); i.src="http://stats.bbc.co.uk/o.gif?~RS~s~RS~iplayer~RS~t~RS~Web_progi~RS~i~RS~b00c3rtd~RS~p~RS~0~RS~a~RS~0~RS~u~RS~/iplayer/page/item/b00c3rtd.shtml~RS~r~RS~http://www.bbc.co.uk/iplayer/search/?q=graham&amp;go=Find+Programmes~RS~q~RS~q=graham&amp;start=1&amp;scope=iplayersearch&amp;go=Find+Programmes&amp;version_pid=b00c3rrt~RS~z~RS~02~RS~";
        #</script>
        chomp( my $url_1b = (grep /i\.src=\"http:\/\/stats\.bbc\.co\.uk\/o\.gif.*b0\w{5}.*\";/, @content)[0] );
        $url_1b =~ s/^.*i\.src=\"(http:\/\/stats\.bbc\.co\.uk\/o\.gif.*b0\w{5}.*)\";.*$/$1/g;
        logger "INFO: Web bug#1: $url_1b\n" if $opt{verbose};

        # Stage 1b - get o.gif web bug to whitelist cookie
        #my $url_1b = 'http://stats.bbc.co.uk/o.gif?~RS~s~RS~iplayer~RS~t~RS~Web_progi~RS~i~RS~b00c3rtd~RS~p~RS~0~RS~a~RS~0~RS~u~RS~/iplayer/page/item/b00c3rtd.shtml~RS~r~RS~(none)~RS~q~RS~q=graham+norton&amp;go=Find+Programmes&amp;scope=iplayersearch&amp;start=1&amp;version_pid=b00c3rrt~RS~z~RS~50~RS~ HTTP/1.1';
	logger "INFO: Getting iplayer 1st web bug              \r";
	#GET /o.gif?~RS~s~RS~iplayer~RS~t~RS~Web_progi~RS~i~RS~b00c3rtd~RS~p~RS~0~RS~a~RS~0~RS~u~RS~/iplayer/page/item/b00c3rtd.shtml~RS~r~RS~(none)~RS~q~RS~q=graham+norton&amp;go=Find+Programmes&amp;scope=iplayersearch&amp;start=1&amp;version_pid=b00c3rrt~RS~z~RS~50~RS~ HTTP/1.1
	#Accept: */*
	#Accept-Language: en
	#Accept-Encoding: gzip, deflate
	#Cookie: BBC-UID=54xxxxxxxxx71ad6e33cfdf040e01b44068765f2a0b061b4447fe92f6528b1ae0Mozilla%2f5%2e0%20%28iPod%3b%20U%3b%20CPU%20like%20Mac%20OS%20X%3b%20en%29
	#Referer: http://www.bbc.co.uk/iplayer/page/item/b00c3rtd.shtml?q=graham+norton&go=Find+Programmes&scope=iplayersearch&start=1&version_pid=b00c3rrt
	#User-Agent: Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3B48b Safari/419.3
	#Connection: keep-alive
	#Host: stats.bbc.co.uk
	my $h = new HTTP::Headers(
		'User-Agent'	=> $user_agent{safari},
		'Accept'	=> '*/*',
	);
	my $req = HTTP::Request->new ('GET', $url_1b, $h);
	# send request
	my $res = $ua->request($req);
	# Get resulting Location header (i.e. redirect URL)
	if ( ! $res->is_success ) {
		logger "ERROR: Failed to get o.gif web bug from iplayer site\n\n";
		return 2;
	}


	# Stage 1c - get o.gif framework web bug to whitelist cookie (5 digit random number appended)
	my $url_1c = $web_bug_2_url.(sprintf "%05.0f", 100000*rand(0));
	logger "INFO: Getting iplayer 2nd web bug             \r";
        #GET /iplayer/framework/img/o.gif?90927 HTTP/1.1
        #Accept: */*
        #Accept-Language: en
        #Accept-Encoding: gzip, deflate
        #Cookie: BBC-UID=e4xxxxxx731e0ec0d34a019ab030cb2de22aef9c407041d4343f4937d42343d40Mozilla%2f5%2e0%20%28iPod%3b%20U%3b%20CPU%20like%20Mac%20OS%20X%3b%20en%29%20AppleWebKit%2f420%2e1%20%28KHTML%2c%20like%20Gecko%29%20Version%2f3%2e0%20Mobile%2f3B48b%20Safari%2f419%2e3
        #Referer: http://www.bbc.co.uk/iplayer/page/item/b00c3rtd.shtml?q=graham+norton&go=Find+Programmes&scope=iplayersearch&start=1&version_pid=b00c3rrt
        #User-Agent: Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3B48b Safari/419.3
        #Connection: keep-alive
        #Host: www.bbc.co.uk
	my $h = new HTTP::Headers(
		'User-Agent'	=> $user_agent{safari},
		'Accept'	=> '*/*',
	);
	my $req = HTTP::Request->new ('GET', $url_1c, $h);
	# send request
	my $res = $ua->request($req);
	# Get resulting Location header (i.e. redirect URL)
	if ( ! $res->is_success ) {
		logger "ERROR: Failed to get 2nd o.gif web bug from iplayer site\n\n";
		return 2;
	}

	return 0;
}



# Usage: moov_length = relocate_moov_chunk_offsets(<binary string>)
sub relocate_moov_chunk_offsets {
	my $moovdata = @_[0];
	# Change all the chunk offsets in moov->stco atoms and add moov_length to them all
	# get moov atom length
	my $moov_length = bytestring_to_int( substr($moovdata, 0, 4) );
	# Nasty but quicker to scan for 'stco'
	for (my $i = 0; $i < $moov_length - 4; $i++) {
		my $chars = substr($moovdata, $i, 4);
		# If we have found and stco atom
		if ( $chars eq 'stco' ) {
			# determine length of atom (4 bytes preceding stco)
			my $stco_len = bytestring_to_int( substr($moovdata, $i-4, 4) );
			logger "INFO: Found stco atom at moov atom offset: $i length $stco_len\n" if $opt{verbose};

			# loop through all chunk offsets in this atom and add offset (== moov atom length)
			for (my $j = $i+12; $j < $stco_len+$i-4; $j+=4) {
				my $chunk_offset = bytestring_to_int( substr($moovdata, $j, 4) );
				#logger "chunk_offset @ $i, $j = '".get_hex( substr($moovdata, $j, 4) )."',	$chunk_offset + $moov_length = ";
				$chunk_offset += $moov_length;
				# write back bytes into $moovdata
				substr($moovdata, $j+0, 1) = chr( ($chunk_offset >> 24) & 0xFF );
				substr($moovdata, $j+1, 1) = chr( ($chunk_offset >> 16) & 0xFF );
				substr($moovdata, $j+2, 1) = chr( ($chunk_offset >>	8) & 0xFF );
				substr($moovdata, $j+3, 1) = chr( ($chunk_offset >>	0) & 0xFF );
				#$chunk_offset = bytestring_to_int( substr($moovdata, $j, 4) );
				#logger "$chunk_offset\n";
			}

			# skip over this whole atom now it is processed
			$i += $stco_len;
		}
	}
	# Write $moovdata back to calling string
	@_[0] = $moovdata;
	return $moov_length;
}



# Usage download_block($file, $url_2, $ua, $start, $end, $file_len, $fh);
#  ensure filehandle $fh is open in append mode
# or, $content = download_block(undef, $url_2, $ua, $start, $end, $file_len);
# Called in 4 ways:
# 1) write to real file			=> download_block($file, $url_2, $ua, $start, $end, $file_len, $fh);
# 2) write to real file + STDOUT	=> download_block($file, $url_2, $ua, $start, $end, $file_len, $fh); + $opt{stdout}==true
# 3) write to STDOUT only		=> download_block($file, $url_2, $ua, $start, $end, $file_len, $fh); + $opt{stdout}==true + $opt{nowrite}==false
# 4) write to memory (and return data)  => download_block(undef, $url_2, $ua, $start, $end, $file_len, undef);
# 4) write to memory (and return data)  => download_block(undef, $url_2, $ua, $start, $end);
sub download_block {

	my ($file, $url, $ua, $start, $end, $file_len, $fh) = @_;
	my $orig_length;
	my $buffer;

	# If this is an 'append to file' mode call
	if ( defined $file && $fh && (!$opt{nowrite}) ) {
		# Stage 3b: Download File
		$orig_length = stat($file)->size;
		logger "INFO: Appending to $file\n" if $opt{verbose};
	}

	# Determine offset for continuation download
	my $h = new HTTP::Headers(
		'User-Agent'	=> $user_agent{coremedia},
		'Accept'	=> '*/*',
		'Range'        => "bytes=${start}-${end}",
	);

	my $req = HTTP::Request->new ('GET', $url, $h);

	# Set time to use for download rate calculation
	# Define callback sub that gets called during download request
	# This sub actually writes to the open output file and reports on progress
	my $callback = sub {
		my ($data, $res, undef) = @_;
		# Don't write the output to the file if there is no content-length header
		return 0 if ( ! $res->header("Content-Length") );
		# Write output
		print $fh $data if ! $opt{nowrite};
		print STDOUT $data if $opt{stdout};
		# return if streaming to stdout - no need for progress
		return if $opt{stdout} && $opt{nowrite};
		return if $opt{quiet};
		# current file size
		my $size = stat($file)->size;
		# download rates in bytes per second
		my $rate = ($size - $orig_length) / (stat($file)->mtime - $now + 0.01);
		# time remaining
		my @time = gmtime( ($file_len - $size) / ($rate + 0.01) );
		printf STDERR "%8.2fMB / %.2fMB %5.0fkbps %5.1f%%, %02d:%02d:%02d remaining         \r", 
			$size / 1024.0 / 1024.0, 
			$file_len / 1024.0 / 1024.0,
			$rate * 8.0 / 1024.0,
			100.0 * $size / $file_len,
			@time[2,1,0],
		;
	};

	my $callback_memory = sub {
		my ($data, $res, undef) = @_;
		# append output to buffer
		$buffer .= $data;
		return if $opt{quiet};
		# current buffer size
		my $size = length($buffer);
		# download rates in bytes per second
		my $rate = $size / ( time() - $now + 0.01 );
		# If we can get COntent_length then display full progress
		if ($res->header("Content-Length")) {
			# Block length
			$file_len = $res->header("Content-Length");
			# time remaining
			my @time = gmtime( ($file_len - $size) / ($rate + 0.01) );
			printf STDERR "%8.2fMB / %.2fMB %5.0fkbps %5.1f%%, %02d:%02d:%02d remaining         \r", 
				$size / 1024.0 / 1024.0, 
				$file_len / 1024.0 / 1024.0,
				$rate * 8.0 / 1024.0,
				100.0 * $size / $file_len,
				@time[2,1,0],
			;

		# Just used simple for if we cannot determine content length
		} else {
			printf STDERR "%8.2fMB %5.0fkbps         \r", $size / 1024.0 / 1024.0, $rate * 8.0 / 1024.0;
		}
	};

	# send request
	logger "\nINFO: Downloading range ${start}-${end}\n" if $opt{verbose};
	logger "\r                              \r";
	my $res;

	# If $fh undefined then get block to memory (fh always defined for stdout or file d/load)
	if (defined $fh) {
		logger "DEBUG: writing stream to stdout, Range: $start - $end of $url\n" if $opt{verbose} && $opt{stdout};
		logger "DEBUG: writing stream to $file, Range: $start - $end of $url\n" if $opt{verbose} && !$opt{nowrite};
		$res = $ua->request($req, $callback);
		if (  (! $res->is_success) || (! $res->header("Content-Length")) ) {
			logger "ERROR: Failed to Download block\n\n";
			unlink $cookiejar;
			return 5;
		}
                # If we don't require any more bytes (previously fully downloaded) then skip
                logger "INFO: Content-Length = ".$res->header("Content-Length")."                               \n" if $opt{verbose};
		return 0;
		   
	# Memory Block
	} else {
		logger "DEBUG: writing stream to memory, Range: $start - $end of $url\n" if $opt{verbose};
		$res = $ua->request($req, $callback_memory);
		if ( (! $res->is_success) ) {
			logger "ERROR: Failed to Download block\n\n";
			return '';
		} else {
			return $buffer;
		}
	}
}


# Converts a string of chars to it's HEX representation
sub get_hex {
        my $buf = shift;
        my $ret;
        for (my $i=0; $i<length($buf); $i++) {
                $ret .= " ".sprintf("%02lx", ord substr($buf, $i, 1) );
        }
	logger "DEBUG: HEX string value = $ret\n" if $opt{verbose};
        return $ret;
}


# Converts a string of chars to it's MSB decimal value
sub bytestring_to_int {
	# Reverse to LSB order
        my $buf = reverse shift;
        my $dec;
        for (my $i=0; $i<length($buf); $i++) {
		# Multiply byte value by 256^$i then accumulate
                $dec += (ord substr($buf, $i, 1)) * 256 ** $i;
        }
        #logger "DEBUG: Decimal value = $dec\n" if $opt{verbose};
        return $dec;
}


# Usage: $fh = open_file_append($filename);
sub open_file_append {
	local *FH;
	my $file = shift;
	# Just in case we actually write to the file - make this /dev/null
	$file = '/dev/null' if $opt{nowrite};
	if ($file) {
		if ( ! open(FH, ">> $file") ) {
			logger "ERROR: Cannot write or append to $file\n\n";
			exit 1;
		}
	}
	# Fix for binary - needed for Windows
	binmode FH;
	return *FH;
}


# Updates and overwrites this script - makes backup as <this file>.old
sub update_script {
	# Get version URL
	my $ua = LWP::UserAgent->new;
	$ua->timeout([10]);
	$ua->proxy( ['http'] => $proxy_url );
	$ua->agent( $user_agent{update} );
	logger "INFO: Checking for latest version from linuxcentre.net\n";
	my $res = $ua->request( HTTP::Request->new( GET => $version_url ) );
	chomp( my $latest_ver = $res->content );
	if ( $res->is_success ) {
		# Compare version numbers
		if ( $latest_ver > $version ) {
			logger "INFO: New version $latest_ver available, downloading\n";
			my $res = $ua->request( HTTP::Request->new( GET => $update_url ) );		
			my $req = HTTP::Request->new ('GET', $update_url);
			# Save content into a $script_file
			my $script_file = $0;
			$ua->request($req, $script_file.'.tmp');
			# If the download was successful then copy over this script and make executable after making a backup of this script
			if ( $res->is_success ) {
				if ( copy($script_file, $script_file.'.old') ) {
					move($script_file.'.tmp', $script_file);
					chmod 0755, $script_file;
					logger "INFO: Copied new version $latest_ver into place (previous version is now called '${script_file}.old')\n";
					# Need to purge the cache as this changes between versions
					unlink($cachefile);
				}
			}
		} else {
			logger "INFO: No update is necessary (latest version = $latest_ver)\n";
		}
	} else {
		logger "ERROR: Failed to connect to update site\n";
		exit 2;
	}
	exit 0;
}


# Creates the Freevo FXD meta data (and pre-downloads graphics - todo)
sub create_fxd() {
	my %table = (
		'A-C' => '[abc]',
		'D-F' => '[def]',
		'G-I' => '[ghi]',
		'J-L' => '[jkl]',
		'M-N' => '[mn]',
		'O-P' => '[op]',
		'Q-R' => '[qt]',
		'S-T' => '[st]',
		'U-V' => '[uv]',
		'W-Z' => '[wxyz]',
		'0-9' => '[\d]',
	);
	if ( open(FXD, "> $fxdfile") ) {
		print FXD '<?xml version="1.0" ?><freevo>';

        # containers for programmes sorted into subdirs
        print FXD "\t<container title=\"iplayer by Programme Name\" type=\"video\">\n";
        my %program_index;
        my %program_count;
        # create hash of programme_name -> index
        for (@_) {
          $program_index{$prog{$index_pid{$_}}{name}} = $_;
          $program_count{$prog{$index_pid{$_}}{name}}++;
        }
        for my $name ( sort keys %program_index ) {
                my @count = grep /^$name$/, keys %program_index;
                print FXD "\t<container title=\"$name ($program_count{$name})\" type=\"video\">\n";
	    	for (@_) {
          	  my $pid = $index_pid{$_};
          	  # loop through and find matches for each progname
	    	  if ( $prog{$index_pid{$_}}{name} =~ /^$name$/ ) {
			my $title = "$prog{$pid}{episode} ($prog{$pid}{available})";
			my $desc = $prog{$pid}{desc};
	    	   	$title =~ s/&/&amp\;/g;
	    	   	$desc =~ s/&/&amp\;/g;
	    	   	  $prog{$pid}{desc} =~ s/&/&amp\;/g;
	    		  print FXD "	<movie title=\"${title}\">
						<video>
							<url id=\"p1\">${pid}.mov<playlist/></url>
						</video>
						<info>
							<description>${desc}</description>
						</info>
					</movie>\n";
                  }
                }			
                print FXD "\t</container>\n";
        }
        print FXD "\t</container>\n";        


        # containers for programmes sorted into channels
        print FXD "\t<container title=\"iplayer by Channel\" type=\"video\">\n";
        my %channels;
        # create hash of channel names
        for (@_) {
          $channels{$prog{$index_pid{$_}}{channel}} = 1;
        }
        for my $channel ( sort keys %channels ) {
        	print FXD "\t<container title=\"$channel\" type=\"video\">\n";
	    	for (@_) {
          		my $pid = $index_pid{$_};
          		# loop through and find matches for each progname
          		if ( $prog{$index_pid{$_}}{channel} =~ /^$channel$/ ) {
				my $title = "$prog{$pid}{name} - $prog{$pid}{episode} ($prog{$pid}{available})";
				my $desc = $prog{$pid}{desc};
				$title =~ s/&/&amp\;/g;
				$desc =~ s/&/&amp\;/g;
				$prog{$pid}{desc} =~ s/&/&amp\;/g;
				print FXD "<movie title=\"${title}\">
						<video>
							<url id=\"p1\">${pid}.mov<playlist/></url>
							</video>
						<info>	
							<description>${desc}</description>
						</info>
					</movie>\n";
			}
                }
                print FXD "\t</container>\n";
        }
        print FXD "\t</container>\n";        


        # containers for alpha sorted programmes
        print FXD "\t<container title=\"iplayer A-Z\" type=\"video\">\n";
        for my $folder (sort keys %table) {
                print FXD "\t<container title=\"iplayer $folder\" type=\"video\">\n";
                for (@_) {
	            	my $pid = $index_pid{$_};
	    	    	my $name = $prog{$pid}{name};
                        my $regex = $table{$folder};
			my $title = "$name - $prog{$pid}{episode} ($prog{$pid}{available})";
			my $desc = $prog{$pid}{desc};
	    	   	$title =~ s/&/&amp\;/g;
	    	   	$desc =~ s/&/&amp\;/g;
	    	    	if ( $name =~ /^$regex/i ) {
	    		  print FXD "	<movie title=\"${title}\">
						<video>
							<url id=\"p1\">${pid}.mov<playlist/></url>
						</video>
						<info>
							<description>${desc}</description>
						</info>
					</movie>\n";
                        }
                }
                print FXD "\t</container>\n";
        }
        print FXD "\t</container>\n";

  } else {
    logger "Couldn't open fxd file $fxdfile for writing\n";
  }
  print FXD '</freevo>';
  close (FXD);
}

sub create_html {
	# Create local web page
	if ( open(HTML, "> $webfile") ) {
	  print HTML '<html><head></head><body><table border=1>';
	  for (@_) {
            my $pid = $index_pid{$_};
	    my $name = $prog{$pid}{name};
	    print HTML "<tr>
	      <td rowspan=2 width=150><a href=\"${prog_page_prefix}/${pid}.html\"><img height=84 width=150 src=\"$prog{$pid}{thumbnail}\"></a></td>
	      <td>$_</td><td><a href=\"${prog_page_prefix}/${pid}.html\">${name}</a></td> <td>$prog{$pid}{episode}</td>
	    </tr>
	    <tr>
	      <td colspan=4>$prog{$pid}{desc}</td>
	    </tr>
	    \n";
	  }
	  print HTML '</table></body>';
	  close (HTML);
	} else {
	  logger "Couldn't open html file $webfile for writing\n";
	}
}


# Save options to file
sub save_options_file {
	unlink $optfile;
	open (OPT, "> $optfile") || die ("ERROR: Cannot save options to $optfile\n");
	# Save all opts except for these
	for (grep !/(help|test|debug)/, keys %opt) {
		print OPT "$_ $opt{$_}\n"  if defined $opt{$_};
	}
	close OPT;
	logger "INFO: Options saved as defult in $optfile\n";
	exit 0;
}


sub read_options_file {
	return 0 if ! -f $optfile;
	open (OPT, "< $optfile") || die ("ERROR: Cannot read options file $optfile\n");
	while(<OPT>) {
		/^\s*([\w\-_]+)\s+(.*)\s*$/;
		chomp( $opt{$1} = $2 );
	}
	close OPT;
}

