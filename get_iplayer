#!/usr/bin/perl
#
# Gets BBC iplayer links
#
# curl "http://www.bbc.co.uk/iplayer/last7days/?filter=txdate%3A12-03&filter=txslot%3Amorning&start=1&scope=iplayerlast7days" 2>/dev/null | grep 'version_pid=' | perl -p -e 's|^.*version_pid=(.+)\">([\w ]+)<\/a>$|http://www.bbc.co.uk/mediaselector/3/auth/iplayer_streaming_http_mp4/$1\|$2|g' | grep -v 7days
#
# download an specific mpeg4 file:
#  curl --cookie-jar /tmp/xxx -vLA 'Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en)' --proxy http://localhost:13128 http://www.bbc.co.uk/mediaselector/3/auth/iplayer_streaming_http_mp4/b008q01d
use strict;
my $ask = 0;
my $download = join ',', @ARGV;
my $download_dir = './nfs/video/TV/iplayer';
my $VALIDATE = 0; # Use with caution - will generate loadsof web requests
my $url_download_prefix = 'http://www.bbc.co.uk/mediaselector/3/auth/iplayer_streaming_http_mp4/';
my $user_agent = 'Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en)';
my $proxy_url = 'http://localhost:13128';
my $cache_secs = 86000;
my $file = '/tmp/iplayer.cache';
my (%links, %daymonth, %slot, %filenames, %urls, %streams, %prog_index);
my $now = time();
my $days = 8;

# Assume the argv is a regex statement if it doesnt contain just numbers/commas
my $download_regex;
if ($download !~ /^[\d,]+$/) {
	$download_regex = $ARGV[0];
}

# Set web proxy env var if required
$ENV{HTTP_PROXY} = $proxy_url if $proxy_url;
$ENV{http_proxy} = $proxy_url if $proxy_url;

# Get straem links from BBC iplayer site or from cache
get_links();


# Print list of streams
my $count = 1;
for (sort keys %links) {
	print "$count: $_ $daymonth{$_} $slot{$_} = $links{$_}\n";
	$streams{$count} = $links{$_};
	$prog_index{$_} = $count;
	# Munge program name to create a filename
	$filenames{$links{$_}} = "$_.avi";
	$filenames{$links{$_}} =~ s/\s/_/g;
	$filenames{$links{$_}} =~ s/[\(\)]//g;
	$filenames{$links{$_}} =~ s/\//-/g;
	$count++;
}

# Generate ugly html index
gen_html("iplayer.html");
#gen_fxd_categorised("iplayer.fxd");

# Get matching programmes
if ($download_regex) {
	$download = '';
	my @matches = sort grep /$download_regex/, keys %links;
	for (@matches) {
		$download .= "$prog_index{$_},";
	}
}

# Do the downloads
if ($download) {
	my @dl = split /[\s,]+/, $download;
	for (@dl) {
		chomp();
		if ( ! -f "$download_dir/$filenames{$streams{$_}}" ) {
			print "Downloading ITEM=$_ FILE=$filenames{$streams{$_}} URL=$streams{$_}\n";
			print "curl --cookie-jar /tmp/xxx -LA \"$user_agent\" $streams{$_} -o $download_dir/$filenames{$streams{$_}}\n";
			system("curl --cookie-jar /tmp/xxx -LA \"$user_agent\" $streams{$_} -o $download_dir/$filenames{$streams{$_}}");
		} else {
			print "ERROR: Already downloaded ITEM=$_ FILE=$filenames{$streams{$_}} URL=$streams{$_}\n";
		}
	} 
}

# Ask user which stream to play
if ($ask) {
	do {
		print "Which stream do you wish to download? ";
		# Get answer from stdin
		chomp( my $selection = <> );
		# Return theplay if it exists in the %streams hash
		if ( defined $streams{$selection} ) {
			print "Playing...\n";
			if ( system("mplayer -user-agent \"$user_agent\" -cookies-file /tmp/xxx -cookies -cache 4096 $streams{$selection}") != 0 ) {
				print "ERROR: Stream could not be played...\n";
			}
		} elsif ( $selection == '0' ) {
			exit 0; 
		} else {
			print "Invalid selection\n";
		}
	} while (1);
}

exit 0;


sub get_links_for_date {
	my ($day, $month, $slot) = @_;
	my $pageno = 1;
	my @page;
	print "Results for $day / $month / $slot:";
	# Loop while we still get stream links
	do {
		print ".";
		chomp( @page = grep /version_pid=\w+">[^<]+<\/a>/, `curl $proxy_url -A "$user_agent" "http://www.bbc.co.uk/iplayer/last7days/?filter=txdate%3A${day}-${month}&filter=txslot%3A${slot}&start=${pageno}&scope=iplayerlast7days" 2>/dev/null` );
		s/^.*version_pid=(\w+)">([^<]+)<\/a>/$1|$2/g for @page;
		for (@page) {
			my ($url, $progname) = split /\|/;
			# Duplicate detection
			if ( not defined $urls{$url_download_prefix.$url} ) {
				$links{"$progname ($day/$month)"} = $url_download_prefix.$url;
				# Other hashes for categorisation
				$daymonth{"$progname ($day/$month)"} = $day."/".$month;
				$slot{"$progname ($day/$month)"} = $slot;
				# Used to detect duplicates
				$urls{$url_download_prefix.$url} = "$progname ($day/$month)";
				
				# Validate link if required
				if ($VALIDATE) {
					print "INFO: Validating stream link for $_\n";
					`curl $proxy_url -A "$user_agent" -m 2 -I $links{"$progname ($day/$month)"} 2>&1 | grep statuscode=not_available >/dev/null 2>&1`;
					if (! $?) {
						delete $links{"$progname ($day/$month)"};
						print "WARNING: removing invalid stream link for \"$progname ($day/$month)\"\n";
					}
				}
			} else {
				print "WARNING: removing duplicate stream link for \"$progname ($day/$month)\"\n";
			}

		}
		# Next page
		$pageno++;
	} while ($#page >= 0);
	print "\n";
}

sub get_links {
	my @cache;

	# Open cache file (need to verify we can even read this)
	if ( open(CACHE, "< $file") ) {
		@cache = <CACHE>;
		close (CACHE);
	}

	# if a cache file doesn't exist/corrupted or original file is older than 2 mins then download new data
	if ( ($cache[0] =~ /^0$/) || (! -f $file) || ($now >= ((stat($file))[9] + $cache_secs)) ) {

		# Get last $days of listings
		for (my $i=0; $i<$days; $i++) {
			chomp( my ($d, $m) = split /\s+/, `date +'%d %m' -d "$i days ago"` );
			for my $s qw(morning afternoon evening) {
				get_links_for_date($d,$m,$s);
			}
		}

		# Open cache file for writing
		if ( open(CACHE, "> $file") ) {
			for (sort keys %links) {
				print CACHE "$_|$links{$_}|$daymonth{$_}|$slot{$_}\n";
			}
			close (CACHE);
			# Make sure anyone can read/write file
			`chmod 777 $file`;
		} else {
			print "Couldn't open cache file for writing\n";
		}

	# Else read from cache
	} else {
		for (@cache) {
			# Populate %links from cache
			chomp();
			my ($progname, $url, $dm, $s) = split /\|/;
                        $links{$progname} = $url;
			$daymonth{$progname} = $dm;
			$slot{$progname} = $s;
			$urls{$url} = $progname;
		}
	}

	return 0;
}

sub gen_html() {
	my $webfile = shift;
	if ( open(HTML, "> $webfile") ) {
		print HTML '<html><head></head><body>';
		for (sort keys %links) {
			print HTML "<a href=\"$links{$_}\">$prog_index{$_} : $_ $daymonth{$_} $slot{$_}</a><br>\n";
		}
		print HTML '</body>';
		close (HTML);
	} else {
		print "Couldn't open html file $webfile for writing\n";
	}
}

sub gen_fxd() {
	my $fxdfile = shift;
	if ( open(HTML, "> $fxdfile") ) {
		print HTML '<?xml version="1.0" ?><freevo>';
		for (sort keys %links) {
			print HTML "<movie title=\"$_\"><video><url id=\"p1\">$links{$_}<playlist/></url></video><info><description>$_</description></info></movie>\n";
		}
		print HTML '</freevo>';
		close (HTML);
	} else {
		print "Couldn't open fxd file $fxdfile for writing\n";
	}
}

sub gen_fxd_categorised() {
	my $fxdfile = shift;
	if ( open(HTML, "> $fxdfile") ) {
		print HTML '<?xml version="1.0" ?>'."\n<freevo>\n";

		# Get last $days of listings
		for (my $i=0; $i<$days; $i++) {
			chomp( my ($d, $m) = split /\s+/, `date +'%d %m' -d "$i days ago"` );
			print HTML "\t<container title=\"$d\/$m\" type=\"video\">\n";
			for (sort keys %links) {
				if ( $daymonth{$_} =~ /^$d\/$m$/ ) {
					print HTML "\t\t<movie title=\"$_ $slot{$_}\"><video><url id=\"p1\">$links{$_}<playlist/></url></video><info><description>$_ $slot{$_}</description></info></movie>\n";
				}
			}
			print HTML "\t</container>\n";
		}
		print HTML '</freevo>';
		close (HTML);
	} else {
		print "Couldn't open fxd file $fxdfile for writing\n";
	}
}
