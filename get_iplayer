#!/usr/bin/perl
#
# get_iplayer
#
# Lists and downloads BBC iplayer mov streams
#
# Author: P Lewis
# Email: iplayer (at sign) linuxcentre.net
# License: GPLv3 (see LICENSE.txt)
# Date: June 11th 2008
# Version: 0.14
#
# Supports: 
# * Downloading Mov streams from BBC Iplayer site - 20080312
# * Automatic XOR decoding of files - 20080609
# * XOR decoding of previously downloaded files - 20080609
# * Indexing of all available (i.e. listed) Iplayer programs (now uses A-Z index page with search set to '*') - 20080609
# * Caching of Index (default 24hrs)
# * Creation of a basic html index file
# * De-duplication of programme index
# * HTTP Proxy support - 20080610
# * Regex search on programme name capability (makes it useful to run this from crontab)
# * Regex search on long programme description capability (-l) - 20080610
# * Tested on Linux (Fedora 6/7/8/9, Centos/RHEL 5, Ubuntu)
#
# Changes 0.14:
# * Not using curl anymore - seems to fail through some firewalls mysteriously and will not work on Ubuntu
# * Using LWP for all http requests
# * Fixed indexing sort bug
#
# Requires:
# * perl 5.8
# * perl LWP

use strict;
use URI;
use IO::Socket;
use vars qw($opt_h $opt_t $opt_d $opt_f $opt_o $opt_p $opt_e $opt_l $opt_n $opt_v $opt_w);
use Getopt::Std;
use Fcntl;
use File::stat;
use IO::Seekable;
use LWP::UserAgent;
#use LWP::Debug qw(+);
use HTTP::Cookies;
use HTTP::Headers;
my $DEBUG = 0;
$|=1;

sub usage {
	print "Usage:\n";
	print "  Download files:        get_iplayer [-l] [-t] [-f] [-n] [-w] [-v] [-e <secs>] [-o <dir>] [-p <proxy_url>] <regex|index|pid|url> [<regex|index|pid|url>]\n";
	print "  Decode Existing Files: get_iplayer [-v] -d <files to decode>\n";
	print "  List Programmes:       get_iplayer [-v] [-l] [-e <secs>] [-p <proxy_url>]\n";
	print "Options:\n";
	print "       -l        show/search long programme descriptions\n";
	print "       -t        test only - no download\n";
	print "       -f        flush cache\n";
	print "       -e <secs> cache expiry in seconds\n";
	print "       -o <dir>  Download output directory\n";
	print "       -p <url>  Web proxy URL spec (only used for parsing iplayer site - not downloads)\n";
	print "       -n        Do not perform XOR decoding\n";
	print "       -d        Perform XOR decoding on specified existing files\n";
	print "       -w        keep whitespace in filenames\n";
	print "       -v        verbose\n";
	print "       -h        help\n";
	exit 1;
}

# Parse options
usage() if (! getopts("htwnvdfo:p:e:l")) || $opt_h;

# Options
my $download_dir 	= $opt_o || "$ENV{HOME}/nfs/video/TV/iplayer";
my $webfile 		= $download_dir.'/iplayer.html';
my $cookiejar		= "/tmp/iplayer_cookie_$$";
my $cachefile		= '/tmp/iplayer.cache';
my $cache_secs 		= $opt_e || 86000;

my $url_download_prefix	= 'http://www.bbc.co.uk/mediaselector/3/auth/iplayer_streaming_http_mp4';
my $prog_page_prefix	= 'http://www.bbc.co.uk/programmes';
my $search_page_prefix	= 'http://www.bbc.co.uk/iplayer/atoz/?filter=azgroup%3A*&start=';
my %ua = (
  	coremedia	=> "Apple iPhone v1.1.1 CoreMedia v1.0.0.3A110a",
  	safari		=> "Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A110a Safari/419.3",
);

# Structures:
#  $links		{'<PROGNAME> (<PID>)'} 	=> <URL>
#  $episodes		{'<PROGNAME> (<PID>)'} 	=> <EPISODE>
#  $descriptions	{'<PROGNAME> (<PID>)'} 	=> <DESCRIPTION>
#  $prog_index		{'<PROGNAME> (<PID>)'} 	=> <INDEX>
#  $streams		{<INDEX>} 		=> <URL>
#  $urls		{<URL>} 		=> '<PROGNAME> (<PID>)'
#  $pids		{<PID>} 		=> 1
my (%links, %urls, %streams, %prog_index, %pids, %episodes, %descriptions);

# Web proxy
my $proxy_opt;
my $proxy_url = $opt_p || $ENV{HTTP_PROXY} || $ENV{http_proxy} || '';
print "INFO: Using Proxy $proxy_url\n" if $proxy_url;

# XOR Decode-only mode
if ($opt_d) {
	for my $file (@ARGV) {
		if ( -f $file ) {
			iplayer_decoder($file);
		} else {
			print "ERROR: Cannot find $file\n";
		}
	}
	exit 0;
}

# Check for valid dload dir
if (! -d $download_dir ) {
	die ("ERROR: $download_dir does not exist\n");
}

# Flush cache if required
unlink ($cachefile) if $opt_f;

# Get stream links from BBC iplayer site or from cache (also populates all hashes)
get_links();

# Print list of streams if we're not downloading anything
if ( ! $ARGV[0] ) {
        for (sort { $a <=> $b } keys %streams) {
                my $prog = $urls{$streams{$_}};
                my $desc = "- $descriptions{$prog}" if $opt_l;
                print "$_: $prog - $episodes{$prog} $desc\n";
        }
        exit 0;
}

# Parse remaining args
my @download_list;
for ( @ARGV ) {
        chomp();
        # If Numerical value
        if ( /^[\d]+$/ ) {
                push @download_list, $_;

        # If PID then find matching programmes with this PID
        } elsif ( /^.*b0[a-z,0-9]{6}.*$/ ) {
                s/^.*(b0[a-z,0-9]{6}).*$/$1/g;
                push @download_list, get_regex_matches( $1 );

        # Else assume this is a programme name regex
        } else {
                push @download_list, get_regex_matches( $_ );
        }
}

# Display list for download
print "Download List:\n" if @download_list;
for (@download_list) {
        my $prog = $urls{$streams{$_}};
        my $desc = "- $descriptions{$prog}" if $opt_l;
        print "$_: $prog - $episodes{$prog} $desc\n";
}

# Exit here if test only mode
exit 0 if $opt_t;

# Do the downloads based on list of index numbers
for (@download_list) {
        my $prog = $urls{$streams{$_}};
        download_stream_lwp( $links{$prog}, "${prog}-$episodes{$prog}" );
}

exit 0;


# Get matching programmes using supplied regex
sub get_regex_matches {

	my $download_regex = shift;
	my @download_list;
	my @matches;

	# Search prognames and episode name
	push @matches, sort grep /$download_regex/i, keys %links;

	# Also search long descriptions if -l is specified
	if ($opt_l) {
		for (keys %links) {
			push @matches, $_ if $descriptions{$_} =~ /$download_regex/i;
		}
	}

	push @download_list, $prog_index{$_} for @matches;

	return @download_list;
}


sub get_links_all {
	my $pageno = 1;
	my $pid;
	my @page;
	print "INFO: Getting Index page\n";
	# Setup User agent
	my $ua = LWP::UserAgent->new;
	$ua->timeout([10]);
	$ua->proxy( ['http'] => $proxy_url );
	$ua->agent( $ua{safari} );

	# Loop while we still get stream links
	my $count = 1;
	do {
		@page = ();
		print "DEBUG: Getting ${search_page_prefix}${pageno}\n" if $DEBUG;

		# Get index pages
		my $res = $ua->request( HTTP::Request->new( GET => "${search_page_prefix}${pageno}" ) );
		my @html = split /\n/, $res->content;
		if ( ! $res->is_success ) {
			print "ERROR: Failed to get programme index page ${pageno} from iplayer site\n";
			exit 2;
		} else {
			print ".";
		}

		# Get the complete URL
		# e.g.:  <a class="resultlink" href="/iplayer/page/item/b00bwky1.shtml?filter=azgroup%3A%2A&amp;start=33&amp;scope=iplayeratoz&amp;version_pid=b00c3kcd">Wild China</a>
		# Get Episode info: Info follows for upto 40 lines after matched line, e.g.
		#                                       <div class="resultSynopsis">
		#                <p class="title">
		#                        <strong>Stake Out</strong>
		#                        <span class="divider">|</span>
		#                        Episode 7
		#                </p>
		#                <p class="description">Children's hidden camera game show that finds out how well kids know their mates. The Prince of Askabar visits Edinb
		#        </div>

		# parse @html array for episode info
		for (my $i=0; $i<$#html; $i++) {
			my $block;
			# If this is a version_pid line (i.e. one with anchor text in it)
			if ( $html[$i] =~ /version_pid=\w+">[^<]+<\/a>/ ) {
				# Need version_pid & url (note these are different pids)
				# Get url
				my $url = $html[$i];
				my $pid = $html[$i];
				$url =~ s/^.*href=\"(.*\.shtml)\?.*version_pid=\w+">[^<]+<\/a>/$1/g;
				$pid =~ s/^.*href=\".*\.shtml\?.*version_pid=(\w+)">[^<]+<\/a>/$1/g;
				chomp($url);
				chomp($pid);
				# get next 40 lines into a single string
				for (my $j=1; $j<=40; $j++) {
					$block .= $html[$i+$j];
					chomp($block);
				}
				$i += 40;
				my $progdata = $block;
				$progdata =~ s!^.*<p class=\"title\">\s*<strong>(.+)</strong>.+</span>\s*([^<]+)</p>.*<p class=\"description\">([^<]+).*$!$1|$2|$3!g;
				# Remove unwanted whitespace
				$progdata =~ s/\s*$//g;
				$progdata =~ s/\s*\|/|/g;
				$progdata =~ s/\|\s*/|/g;
				print "url|pid|title|episode|desc = '$url|$pid|$progdata'\n" if $opt_v;
				push @page, "$url|$pid|$progdata";
			}
		}

		for (@page) {
			my ($url, $pid, $progname, $episode, $desc) = split /\|/;
			#$url = $url_download_prefix.$url;
			# Get the pid for duplicate detection
			# Duplicate detection
			if ( not defined $pids{$pid} ) {
				# Used to detect duplicates
				$pids{$pid} = 1;
				# Other hashes for categorisation
				$links{"$progname ($pid)"} = $url;
				$streams{$count} = $url;
				$prog_index{"$progname ($pid)"} = $count;
				$urls{$url} = "$progname ($pid)";
				$episodes{"$progname ($pid)"} = $episode;
				$descriptions{"$progname ($pid)"} = $desc;
				print "  $progname - $episode - $desc\n" if $opt_v;
				$count++;
				# Link Validation here...
			} else {
				print "WARNING: removing duplicate stream link for \"$progname ($episode)\" / $pid\n" if $opt_v;
			}

		}
		# Next page
		$pageno++;
	} while ($#page >= 0);
	print "\n";
	return 0;
}


sub get_links {
	my @cache;
	my $now = time();

	# Open cache file (need to verify we can even read this)
	if ( open(CACHE, "< $cachefile") ) {
		@cache = <CACHE>;
		close (CACHE);
	}

	# if a cache file doesn't exist/corrupted or original file is older than 2 mins then download new data
	if ( ($cache[0] =~ /^0$/) || (! -f $cachefile) || ($now >= ( stat($cachefile)->mtime + $cache_secs )) ) {

		# Use A-Z,0-9 searchine '*' instead - less pages
		get_links_all();

		# Open cache file for writing
		if ( open(CACHE, "> $cachefile") ) {
			for (sort {$a <=> $b} keys %streams) {
				my $prog = $urls{$streams{$_}};
				print CACHE "$_|$prog|$links{$prog}|$episodes{$prog}|$descriptions{$prog}\n";
			}
			close (CACHE);
			# Make sure anyone can read/write file (security risk here!!)
			chmod 0777, $cachefile;
		} else {
			print "Couldn't open cache file for writing\n";
		}


	# Else read from cache
	} else {
		for (@cache) {
			# Populate %links from cache
			chomp();
			my ($count, $progname, $url, $episode, $desc) = split /\|/;
			$links{$progname} = $url;
			$streams{$count} = $url;
			$prog_index{$progname} = $count;
			$urls{$url} = $progname;
			$episodes{$progname} = $episode;
			$descriptions{$progname} = $desc;
		}
	}

	# Create local web page
	if ( open(HTML, "> $webfile") ) {
	  print HTML '<html><head></head><body><table border=1>';
	  for (sort keys %links) {
	    # Extract pid from progname
	    my $pid = $_;
	    $pid =~ s/^.*\((b00.....)\).*$/$1/g;
	    my $title = $_;
	    $title =~ s/^(.*)\s*\(.*$/$1/g;
	    print HTML "<tr>
	      <td rowspan=2 width=150><a href=\"${prog_page_prefix}/${pid}.html\"><img height=84 width=150 src=\"http://www.bbc.co.uk/iplayer/images/episode/${pid}_150_84.jpg\"></a></td>
	      <td><a href=\"${prog_page_prefix}/${pid}.html\">${title}</a></td> <td>$episodes{$_}</td>
	    </tr>
	    <tr>
	      <td colspan=4>$descriptions{$_}</td>
	    </tr>
	    \n";
	  }
	  print HTML '</table></body>';
	  close (HTML);
	} else {
	  print "Couldn't open html file $webfile for writing\n";
	}
	

	return 0;
}


# Usage: download_stream_lwp (<url>|<filename_prefix>)
sub download_stream_lwp {

	my $id = shift;
	my $file = shift;

	$id = $1 if $id =~ /\/(b.*?)\.shtml/;
	my $page = "http://www.bbc.co.uk/iplayer/page/item/$id.shtml";

	# Remove spaces and brackets and change to underscores
	$file =~ s/[\\\/]/_/g;
	$file =~ s/\s/_/g if ! $opt_w;
	$file =~ s/[\(\)]//g if ! $opt_w;
	$file = "${download_dir}/${file}.mov";
		
	if ( -f $file ) {
		print "ERROR: File $file already exists - skipping....\n";
		return 0;
	} else {
		print "INFO: Attempting to Download $file\n";
		print "INFO: Stage 1 URL = $page\n" if $opt_v;
		print "\rGetting iplayer programme page        " if ! $opt_v;

		# Setup user agent
		# Switch off automatic redirects
		my $ua = LWP::UserAgent->new( requests_redirectable => [ 'POST' ] );
		# 30 second request timeout
		$ua->timeout([30]);
		$ua->proxy( ['http'] => $proxy_url );
		$ua->cookie_jar( HTTP::Cookies->new( file => $cookiejar, autosave => 1, ignore_discard => 0 ) );
		

		# Stage 1: get PID and set cookie
		$ua->agent( $ua{safari} );
		# send request
		my $res = $ua->request( HTTP::Request->new( GET => $page ) );
		# Extract pid and type (i.e. Signed, Original, etc)
		chomp( my ($type, $pid_1) = (grep /(type|pid )      :/, (split /\n/, $res->content) )[0,1] );
		if ( ! $res->is_success ) {
			print "ERROR: Failed to get programme ID from iplayer site\n";
			return 1;
		}
		# Extract pid from e.g.: "    pid       : 'b00c0rmy',"
		$pid_1 =~ s/^.*\'(b0[a-z,0-9]{6})\'.*$/$1/g;
		$type =~ s/^.*type\s+:(.+)\s*$/$1/g;
		my $url_1 = "${url_download_prefix}/$pid_1";
		print "INFO: Stage 2 Type = $type\n" if $opt_v;
		print "INFO: Stage 2 URL = $url_1\n" if $opt_v;
		print "\rGetting iplayer download URL         " if ! $opt_v;


		# Stage 2: e.g. "Location: http://download.iplayer.bbc.co.uk/iplayer_streaming_http_mp4/121285241910131406.mp4?token=iVXexp1yQt4jalB2Hkl%2BMqI25nz2WKiSsqD7LzRmowrwXGe%2Bq94k8KPsm7pI8kDkLslodvHySUyU%0ApM76%2BxEGtoQTF20ZdFjuqo1%2B3b7Qmb2StOGniozptrHEVQl%2FYebFKVNINg%3D%3D%0A"
		my $h = new HTTP::Headers(
			'User-Agent'	=> $ua{coremedia},
			'Accept'	=> '*/*',
			'Range'		=> 'bytes=0-1',
		);
		my $req = HTTP::Request->new ('GET', $url_1, $h);
		# send request
		my $res = $ua->request($req);
		# Get resulting Location header (i.e. redirect URL)
		my $url_2 = $res->header("location");
		if ( ! $res->is_redirect ) {
			print "ERROR: Failed to get redirect from iplayer site\n";
			return 2;
		}
		# Extract redirection Location URL
		$url_2 =~ s/^Location: (.*)$/$1/g;
		print "INFO: Stage 3 URL = $url_2\n" if $opt_v;


		# Stage 3: Download File
		my $h = new HTTP::Headers(
			'User-Agent'	=> $ua{coremedia},
			'Accept'	=> '*/*',
			'Range'		=> 'bytes=0-',
		);
		my $req = HTTP::Request->new ('GET', $url_2, $h);
		# Set time to use for download rate calculation
		my $now = time();
		# Define callback sub that gets called during download request
		# This sub actually writes to the open output file and reports on progress
		my $callback = sub { 
			my ($data, $res, undef) = @_;
			print FILE $data;
			my $size = stat($file)->size;
			printf( "%8.3f MB %10.3f kbps %5.2f%%   \r", ($size/1024.0/1024.0), $size / (stat($file)->mtime - $now + 0.01) / 1024.0, (100.0*$size/$res->header("Content-Length")) );
		};
		if ( ! open(FILE, "> $file") ) {
			print("ERROR: Cannot write to $file\n");
			return 4;
		}
		# send request (turn off autoflush o/wise we have too many terminal updates)
		$|=0;
		print "\r                              \r";
		my $res = $ua->request($req, $callback);
		$|=1;
		close FILE;
		if (! $res->is_success) {
			print "ERROR: Failed to Download $file\n";
			unlink $cookiejar;
			return 3;
		}
	}

	# XOR Decode if required
	iplayer_decoder($file) if ! $opt_n;

	return 0;
}



# In-place modification of an iplayer file to do that dodgy xor stuff
#
# Usage: iplayer_decode <filename>
#
sub iplayer_decoder {

  chomp( my $file = shift );
  if (! -f $file) {
	print "ERROR: File $file does not exist - no decoding performed\n";
  	return 1;
  }

  my $length =  stat($file)->size;
  #print "$file length = $length\n";

  if (! open (FILE, "+<$file") ) {
  	print "ERROR: Can't update $file: $!";
  	return 1;
  }
  my $buffer;

  # Use preferred blocksize
  my $recsize = stat($file)->blksize;

  # Create long string of '0x3c53's
  my $xor = create_xor_string($recsize/2, 0x3c, 0x53);

  # Offsets
  my $start_offset = 0x2800;
  my $end_offset = 0x400 + 2;

  # Put file ptr in correct place
  seek(FILE, $start_offset, SEEK_SET);

  print "INFO: Only XORing $recsize bytes from $start_offset\n";

  # Do the xoring
  for (my $i = $start_offset; $i < ($length - $end_offset); $i += $recsize) {

	# Make sure we don't touch last 0x402 bytes
	if ( $i > ($length - $end_offset - $recsize) ) {
		$recsize = $length - $end_offset - $i;
		$xor = create_xor_string($recsize/2, 0x3c, 0x53);
		print "\nINFO: Only XORing $recsize bytes from $i\n";
	}

	read(FILE, $buffer, $recsize) == $recsize
		|| die "\nReading: $!";

	# Do the XOR
	$buffer = $buffer ^ $xor;

	# Rewind $buffer bytes then rewrite xor'ed data
	seek(FILE, -1 * $recsize, SEEK_CUR);
	print FILE $buffer;

	printf "\r%0.2f%%", $i/$length*100.0;
	printf "\r%0.2f%%, offset = %X, blksize = %d bytes, (length = %X, start = %X, end = %X)", $i/$length*100.0, $i, $recsize, $length, $start_offset, $length - $end_offset if $opt_v;
  }

  # Last two bytes (length-0x402)->(length-0x401) have xor pattern swapped
  my $xor = create_xor_string(1, 0x53, 0x3c);
  print "\nINFO: XORing 2 bytes from ".($length - $end_offset)."\n";
  seek(FILE, $length - $end_offset, SEEK_SET);
  read(FILE, $buffer, 2) == 2 || die "Reading: $!";

  # Do the XOR using 0x53,0x3c
  $buffer = $buffer ^ $xor;

  # Rewind $buffer bytes then rewrite xor'ed data
  seek(FILE, -2, SEEK_CUR);
  print FILE $buffer;

  close FILE || die "Closing: $!";
}


# Build XOR pattern (there has gotta be a more efficient way of doing this)
sub create_xor_string {
	my $count = shift;

	my $pattern;
	$pattern .= chr($_) for @_;

	my $ret;
	for (my $i = 0; $i<$count; $i++) { 
		$ret .= $pattern;
	}
	return $ret; 
}
